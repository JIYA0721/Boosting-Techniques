{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#THEORY ANSWERS\n",
        "\n",
        "1. What is Boosting in Machine Learning? Explain how it improves weak\n",
        "learners.\n",
        "   - Boosting is an ensemble method that combines multiple weak learners sequentially to form a strong model. Initially, all samples have equal weights; after each iteration, weights of misclassified samples increase so the next learner focuses on them. Final predictions are made using weighted voting or averaging. This approach reduces bias and improves accuracy, transforming weak learners into a strong predictive model. Popular algorithms implementing boosting include AdaBoost, Gradient Boosting, XGBoost, LightGBM, and CatBoost.\n",
        "\n",
        "2. What is the difference between AdaBoost and Gradient Boosting in terms\n",
        "of how models are trained?\n",
        "   - The main difference between AdaBoost and Gradient Boosting lies in how they train models and handle errors.\n",
        "\n",
        "* AdaBoost (Adaptive Boosting) trains weak learners sequentially by adjusting the weights of training samples. Misclassified samples get higher weights so that the next weak learner focuses more on those difficult cases. The final model combines learners based on their weighted votes.\n",
        "\n",
        "* Gradient Boosting, on the other hand, trains models by optimizing a loss function using gradient descent. Instead of reweighting samples, it fits each new weak learner to the residual errors (negative gradients) of the previous model, gradually reducing the overall loss.\n",
        "\n",
        "In short, AdaBoost emphasizes sample weighting, while Gradient Boosting emphasizes minimizing loss through gradient-based optimization.\n",
        "\n",
        "3. How does regularization help in XGBoost?\n",
        "   - Regularization in XGBoost helps prevent overfitting and improves the generalization ability of the model by penalizing model complexity. XGBoost includes two types of regularization:\n",
        "\n",
        "L1 Regularization (Lasso) – Encourages sparsity in the model by shrinking less important feature weights toward zero, effectively performing feature selection.\n",
        "L2 Regularization (Ridge) – Penalizes large weights to keep the model stable and prevent over-reliance on specific features.\n",
        "\n",
        "These penalties are applied to the leaf weights of the decision trees in the objective function, which controls tree complexity and prevents the model from becoming too deep or overly specific to the training data. This leads to more robust and generalizable predictions.\n",
        "\n",
        "4. Why is CatBoost considered efficient for handling categorical data?\n",
        "   - CatBoost is efficient for handling categorical data because it uses **target-based encoding with ordered boosting**, which converts categories into numeric values based on target statistics without causing data leakage. It processes categorical features internally, avoiding one-hot encoding and high dimensionality, and handles missing values natively. This makes it accurate, fast, and less prone to overfitting on datasets with many categorical features.\n",
        "\n",
        "5. What are some real-world applications where boosting techniques are\n",
        "preferred over bagging methods?\n",
        "   - Boosting is preferred over bagging in applications requiring high accuracy and low bias, such as:\n",
        "\n",
        "* Credit Scoring & Fraud Detection – Captures subtle patterns in imbalanced financial data.\n",
        "* Search Engine Ranking – Improves relevance of results (e.g., Gradient Boosting in ranking algorithms).\n",
        "* Recommendation Systems – Predicts user preferences accurately.\n",
        "* Healthcare Diagnostics – Disease prediction where accuracy is critical.\n",
        "* Customer Churn Prediction – Detects patterns leading to churn.\n",
        "* Financial Market Forecasting – For precise risk modeling and predictions.\n",
        "\n",
        "These use cases favor boosting because it reduces bias and improves predictive power more effectively than bagging methods like Random Forest.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r1lAwmmhctkE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-XQ-5SWcok8",
        "outputId": "6024668f-0489-4af9-9b30-46bc6806958c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9649122807017544\n"
          ]
        }
      ],
      "source": [
        "# Write a Python program to:\n",
        "# ● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "# ● Print the model accuracy\n",
        "\n",
        "# Python Program to train an AdaBoost Classifier on Breast Cancer dataset\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train AdaBoost classifier\n",
        "model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python program to:\n",
        "# ● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "# ● Evaluate performance using R-squared score\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Gradient Boosting Regressor\n",
        "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"R-squared Score:\", r2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZAsrIiefiTN",
        "outputId": "0ae8d2f2-0e15-4084-fbe4-1864bd577323"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared Score: 0.7756446042829697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python program to:\n",
        "# ● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "# ● Tune the learning rate using GridSearchCV\n",
        "# ● Print the best parameters and accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize XGBoost Classifier\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Define parameter grid for learning rate\n",
        "param_grid = {'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]}\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "# Accuracy score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHURTbJ1f__H",
        "outputId": "8941e292-81db-4808-9b5e-0d16db61a14a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'learning_rate': 0.2}\n",
            "Test Accuracy: 0.956140350877193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [11:20:37] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python program to:\n",
        "# ● Train a CatBoost Classifier\n",
        "# ● Plot the confusion matrix using seaborn\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize CatBoost Classifier\n",
        "model = CatBoostClassifier(iterations=200, learning_rate=0.1, depth=6, verbose=0, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix using seaborn\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=data.target_names,\n",
        "            yticklabels=data.target_names)\n",
        "plt.title('Confusion Matrix - CatBoost Classifier')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Yk-DfjdUhN7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You're working for a FinTech company trying to predict loan default using\n",
        "customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "● Hyperparameter tuning strategy\n",
        "● Evaluation metrics you'd choose and why\n",
        "● How the business would benefit from your model\n",
        "\n",
        "Data Preprocessing\n",
        "\n",
        "* Handle missing values: keep `NaN` (CatBoost/XGBoost handle natively) + missing flags.\n",
        "* Categorical features: use CatBoost (native support) or target encoding for XGBoost.\n",
        "* No scaling needed for trees.\n",
        "* Handle imbalance using class weights or `scale_pos_weight`.\n",
        "\n",
        "Choice of Algorithm\n",
        "\n",
        "* **CatBoost** preferred for mixed numeric/categorical features (handles both well, reduces preprocessing).\n",
        "* XGBoost for numeric-heavy datasets.\n",
        "* AdaBoost only as a baseline.\n",
        "\n",
        "Hyperparameter Tuning\n",
        "\n",
        "* Use Randomized Search → Bayesian/Optuna.\n",
        "* Key params: `learning_rate`, `n_estimators`, `depth`, `subsample`, L1/L2 regularization.\n",
        "* Apply early stopping.\n",
        "\n",
        "Evaluation Metrics\n",
        "\n",
        "* Primary: PR-AUC (best for imbalance).\n",
        "* Secondary: ROC-AUC, Precision\\@K, Recall\\@K, F1-score.\n",
        "* Use cost-sensitive thresholding for business goals.\n",
        "\n",
        "Business Benefits\n",
        "\n",
        "* Reduce loan defaults → lower credit losses.\n",
        "* Enable risk-based pricing and better approval decisions.\n",
        "* Support compliance with explainable predictions (SHAP).\n",
        "* Improve collections efficiency and capital optimization.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9z0Qjn1vhq6T"
      }
    }
  ]
}